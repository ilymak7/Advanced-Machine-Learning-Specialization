{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nfrom tqdm import tqdm\nimport math\nimport gc\nimport matplotlib.pyplot as plt\nimport sklearn\nimport scipy.sparse \nfrom sklearn.impute import KNNImputer\nfrom scipy.stats import t\nfrom sklearn.preprocessing import LabelEncoder\nimport os\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport  lightgbm as lgbm\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-11T13:25:10.460332Z","iopub.execute_input":"2022-02-11T13:25:10.460633Z","iopub.status.idle":"2022-02-11T13:25:10.467439Z","shell.execute_reply.started":"2022-02-11T13:25:10.460602Z","shell.execute_reply":"2022-02-11T13:25:10.466443Z"},"trusted":true},"execution_count":308,"outputs":[]},{"cell_type":"code","source":"maindir = \"../input/zindi-blood-ai\" # Directory with your files\ntraincsv = maindir+\"/Update_train.csv\"\ntestcsv = maindir+\"/Updated_Test.csv\"\ntrain = pd.read_csv('../input/zindi-blood-ai/Update_train.csv')\ntest = pd.read_csv('../input/zindi-blood-ai/Updated_Test.csv')\ntargets = ['hdl_cholesterol_human','hemoglobin(hgb)_human','cholesterol_ldl_human']\n# train.loc[train[targets[0]]=='low',targets[0]] = 'not_ok'\n# train.loc[train[targets[0]]=='ok',targets[0]] = 0\n# train.loc[train[targets[0]]=='high',targets[0]] = 'not_ok'\n# train.loc[train[targets[1]]=='low',targets[1]] = -1\n# train.loc[train[targets[1]]=='ok',targets[1]] = 0\n# train.loc[train[targets[1]]=='high',targets[1]] = 1\n# train.loc[train[targets[2]]=='low',targets[2]] = -1\n# train.loc[train[targets[2]]=='ok',targets[2]] = 0\n# train.loc[train[targets[2]]=='high',targets[2]] = 1\ntrain['target'] = train[targets[0]]\n#.agg('_'.join, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:25:10.823594Z","iopub.execute_input":"2022-02-11T13:25:10.823937Z","iopub.status.idle":"2022-02-11T13:25:11.913110Z","shell.execute_reply.started":"2022-02-11T13:25:10.823904Z","shell.execute_reply":"2022-02-11T13:25:11.912231Z"},"trusted":true},"execution_count":309,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_ing(df):\n#     means = df.groupby('donation_id')[fea].agg(np.mean).reset_index()\n#     stds = df.groupby('donation_id')[fea].agg(np.std).reset_index()\n#     maxs = df.groupby('donation_id')[fea].agg(np.max).reset_index()\n#     mins = df.groupby('donation_id')[fea].agg(np.min).reset_index()\n    absorb = ['absorbance'+str(i) for i in range(0,170)]\n    mins1 = df.groupby('donation_id')[absorb[:17]].agg(min).min(axis = 1).reset_index()\n    idxmins = df.groupby('donation_id')[absorb[:17]].agg(min).idxmin(axis = 1).reset_index()\n#\n    maxs1 = df.groupby('donation_id')[absorb[:17]].agg(max).max(axis = 1).reset_index()\n    idxmaxs = df.groupby('donation_id')[absorb[:17]].agg(max).idxmax(axis = 1).reset_index()\n#     temps = \n#     print(df.columns)\n#          temps = \n#     print(df.columns)\n    for i in range(2,11):\n        \n        mins1 = mins1.merge(df.groupby('donation_id')[absorb[17*(i-1):17*i]].agg(min).min(axis = 1).reset_index(),on = 'donation_id', how = 'left' , suffixes = (None,'_window'+str(i)))    \n        idxmins = idxmins.merge(df.groupby('donation_id')[absorb[17*(i-1):17*i]].agg(min).idxmin(axis = 1).reset_index(),on = 'donation_id', how = 'left' , suffixes = (None,'_window'+str(i)))    \n#        res = res.merge(df[['donation_id','target','std']].copy().drop_duplicates(ignore_index = True),on = 'donation_id', how = 'left') \n        maxs1 = maxs1.merge(df.groupby('donation_id')[absorb[17*(i-1):17*i]].agg(max).max(axis = 1).reset_index(),on = 'donation_id', how = 'left' , suffixes = (None,'_window'+str(i)))    \n        idxmaxs = idxmaxs.merge(df.groupby('donation_id')[absorb[17*(i-1):17*i]].agg(max).idxmax(axis = 1).reset_index(),on = 'donation_id', how = 'left' , suffixes = (None,'_window'+str(i)))    \n    res = maxs1.merge(idxmaxs,on = 'donation_id', how = 'left', suffixes = ('_max','_idx_max'))\n    res1 = mins1.merge(idxmins,on = 'donation_id', how = 'left', suffixes = ('_min','_idx_min'))\n    res = res.merge(res1,on = 'donation_id',how = 'left' )\n    res = res.merge(df[['donation_id','target','std']].copy().drop_duplicates(ignore_index = True),on = 'donation_id', how = 'left') \n    \n    temp_maxidx =  df.groupby('donation_id')[['temperature','humidity']].apply(lambda x : x.idxmax(axis = 0)%60).reset_index()\n    temp_max = df.groupby('donation_id')[['temperature','humidity']].agg(max).reset_index()\n    temp_minidx =  df.groupby('donation_id')[['temperature','humidity']].apply(lambda x : x.idxmin(axis = 0)%60).reset_index()\n    temp_min = df.groupby('donation_id')[['temperature','humidity']].agg(min).reset_index()\n    temp_max = temp_max.merge(temp_maxidx,on = 'donation_id',how = 'left')\n    temp_min = temp_min.merge(temp_minidx,on = 'donation_id',how = 'left')\n    temp =   temp_max.merge(temp_min,on = 'donation_id',how = 'left')\n    res = res.merge(temp, on = 'donation_id', how = 'left')\n    #   res = df[['donation_id','target']].copy().drop_duplicates(ignore_index = True)\n#     res = res.merge(means,on = 'donation_id',how = 'left',suffixes = (None,'_mean')).merge(stds,on = 'donation_id',how = 'left',suffixes = (None,'_std')).merge(maxs,on = 'donation_id',how = 'left',suffixes = (None,'_max')).merge(mins,on = 'donation_id',how = 'left',suffixes = (None,'_min'))\n    return res","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:25:11.914618Z","iopub.execute_input":"2022-02-11T13:25:11.915136Z","iopub.status.idle":"2022-02-11T13:25:11.931867Z","shell.execute_reply.started":"2022-02-11T13:25:11.915099Z","shell.execute_reply":"2022-02-11T13:25:11.930837Z"},"trusted":true},"execution_count":310,"outputs":[]},{"cell_type":"code","source":"\n\n# df\n# feature_ing(train).info()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:25:12.923852Z","iopub.execute_input":"2022-02-11T13:25:12.924126Z","iopub.status.idle":"2022-02-11T13:25:12.928184Z","shell.execute_reply.started":"2022-02-11T13:25:12.924096Z","shell.execute_reply":"2022-02-11T13:25:12.927195Z"},"trusted":true},"execution_count":311,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fea = ['absorbance'+str(i) for i in range(0,170)]+['std','temperature','humidity']\n# train = feature_ing(train,fea)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:25:14.101784Z","iopub.execute_input":"2022-02-11T13:25:14.102226Z","iopub.status.idle":"2022-02-11T13:25:14.107359Z","shell.execute_reply.started":"2022-02-11T13:25:14.102192Z","shell.execute_reply":"2022-02-11T13:25:14.105805Z"},"trusted":true},"execution_count":312,"outputs":[]},{"cell_type":"code","source":"means_col = ['absorbance'+str(i) for i in range(0,10)]+['std','temperature','humidity']\nmaxs_col = ['absorbance'+str(i)+'_max' for i in range(0,10)]+['std_max','temperature_max','humidity_max']\nmins_col = ['absorbance'+str(i)+'_min' for i in range(0,10)]+['std_min','temperature_min','humidity_min']\nstds_col = ['absorbance'+str(i)+'_std' for i in range(0,10)]+['std_std','temperature_std','humidity_std']\nall_cols = means_col +mins_col + maxs_col + stds_col ","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:25:14.641070Z","iopub.execute_input":"2022-02-11T13:25:14.641353Z","iopub.status.idle":"2022-02-11T13:25:14.647298Z","shell.execute_reply.started":"2022-02-11T13:25:14.641323Z","shell.execute_reply":"2022-02-11T13:25:14.646391Z"},"trusted":true},"execution_count":313,"outputs":[]},{"cell_type":"code","source":"# fea\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:25:15.529876Z","iopub.execute_input":"2022-02-11T13:25:15.530144Z","iopub.status.idle":"2022-02-11T13:25:15.543183Z","shell.execute_reply.started":"2022-02-11T13:25:15.530115Z","shell.execute_reply":"2022-02-11T13:25:15.542257Z"},"trusted":true},"execution_count":314,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targ_fea = []\n# fea = train.columns\n# ['absorbance'+str(i) for i in range(10)]+['std','temperature','humidity']\n# def feature_ing(df,fea,targ = True):\n    \n#     means = df.groupby('donation_id')[fea].agg(np.mean).reset_index()\n#     stds = df.groupby('donation_id')[fea].agg(np.std).reset_index()\n#     maxs = df.groupby('donation_id')[fea].agg(np.max).reset_index()\n#     mins = df.groupby('donation_id')[fea].agg(np.min).reset_index()\n#     cumsums = df.groupby('donation_id').agg(np.mean)[fea[:-3]].cumsum(axis = 1)\n#     cumprods = df.groupby('donation_id').agg(np.mean)[fea[:-3]].cumprod(axis = 1)\n#     if targ:\n#         res = df[['donation_id','target']+targ_fea].copy().drop_duplicates(ignore_index = True)\n#     else:\n#         res = df[['donation_id','target']].copy().drop_duplicates(ignore_index = True)\n#     res = res.merge(means,on = 'donation_id',how = 'left',suffixes = (None,'_mean')).merge(stds,on = 'donation_id',how = 'left',suffixes = (None,'_std')).merge(maxs,on = 'donation_id',how = 'left',suffixes = (None,'_max')).merge(mins,on = 'donation_id',how = 'left',suffixes = (None,'_min')).merge(cumprods,on = 'donation_id',how = 'left',suffixes = (None,'_cumprod')).merge(cumsums,on = 'donation_id',how = 'left',suffixes = (None,'_cumsum'))\n#     return res\n# df = train.copy()\ntest['target'] = 'ok'\ntrain = feature_ing(train)\n# df = test.copy()\ntest = feature_ing(test)\nmeans_col = ['absorbance'+str(i) for i in range(0,170)]+['std','temperature','humidity']\nmaxs_col = ['absorbance'+str(i)+'_max' for i in range(0,170)]+['std_max','temperature_max','humidity_max']\nmins_col = ['absorbance'+str(i)+'_min' for i in range(0,170)]+['std_min','temperature_min','humidity_min']\nstds_col = ['absorbance'+str(i)+'_std' for i in range(0,170)]+['std_std','temperature_std','humidity_std']\ncumprods_col = ['absorbance'+str(i)+'_cumprod' for i in range(0,170)]\ncumsums_col = ['absorbance'+str(i)+'_cumsum' for i in range(0,170)]\n\nall_cols = means_col +mins_col + maxs_col + stds_col + cumprods_col + cumsums_col# train[fea]\n# t_h = ['temperature','humidity']\n# train[t_h]\n# test['target'] = 'ok'\n# train = feature_ing(train,fea)\n# test = feature_ing(test,fea)\nnum_fea = train.drop(['donation_id','target'],axis = 1).select_dtypes(include = 'float').columns.tolist()\ncat_fea = train.drop(['donation_id','target'],axis = 1).select_dtypes(include = 'object').columns.tolist()\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.decomposition import PCA\n# pca = PCA(n_components = 0.999999)\n# X = pca.fit_transform(train[fea])\n# # RobustScaler is less prone to outliers.\n# test_X = pca.transform(train[fea])\n# y  = train['target']\n# d_id = train['donation_id']\n# d_id_tes = test['donation_id']\n# train = pd.DataFrame(X,columns = ['feat_'+str(i) for i in range(88)])\n# test = pd.DataFrame(test_X,columns = ['feat_'+str(i) for i in range(88)])\n# train['target'] = y\n# train['donation_id'] = d_id\n# test['donation_id'] = d_id_tes\n# fea = train.columns.tolist()[:-2]\nrob_scaler = RobustScaler()\ntrain[num_fea] = rob_scaler.fit_transform(train[num_fea].values)\ntest[num_fea] = rob_scaler.transform(test[num_fea])\nfrom sklearn.preprocessing import LabelEncoder\nfor f in cat_fea:\n    lab_enc = LabelEncoder()\n    train[f] = lab_enc.fit_transform(train[f])\n    test[f] = lab_enc.transform(test[f])","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:25:17.203257Z","iopub.execute_input":"2022-02-11T13:25:17.203864Z","iopub.status.idle":"2022-02-11T13:25:18.764267Z","shell.execute_reply.started":"2022-02-11T13:25:17.203811Z","shell.execute_reply":"2022-02-11T13:25:18.763507Z"},"trusted":true},"execution_count":315,"outputs":[]},{"cell_type":"code","source":"drop_fea = train.isna().sum()[train.isna().sum()>0].index.tolist()\ntrain = train.drop(drop_fea, axis = 1)\ntest = test.drop(drop_fea, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:25:18.766043Z","iopub.execute_input":"2022-02-11T13:25:18.766371Z","iopub.status.idle":"2022-02-11T13:25:18.782807Z","shell.execute_reply.started":"2022-02-11T13:25:18.766328Z","shell.execute_reply":"2022-02-11T13:25:18.781861Z"},"trusted":true},"execution_count":316,"outputs":[]},{"cell_type":"code","source":"# fea = ","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:25:19.435520Z","iopub.execute_input":"2022-02-11T13:25:19.436460Z","iopub.status.idle":"2022-02-11T13:25:19.441079Z","shell.execute_reply.started":"2022-02-11T13:25:19.436415Z","shell.execute_reply":"2022-02-11T13:25:19.439927Z"},"trusted":true},"execution_count":317,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain['target'] = le.fit_transform(train['target'])\n# train.drop(targets, axis =1 , inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:25:20.099013Z","iopub.execute_input":"2022-02-11T13:25:20.099455Z","iopub.status.idle":"2022-02-11T13:25:20.106077Z","shell.execute_reply.started":"2022-02-11T13:25:20.099422Z","shell.execute_reply":"2022-02-11T13:25:20.105091Z"},"trusted":true},"execution_count":318,"outputs":[]},{"cell_type":"code","source":"# train = reduce_mem_usage(train)\n# train.info()\ntrain.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:25:20.664193Z","iopub.execute_input":"2022-02-11T13:25:20.664496Z","iopub.status.idle":"2022-02-11T13:25:20.682800Z","shell.execute_reply.started":"2022-02-11T13:25:20.664460Z","shell.execute_reply":"2022-02-11T13:25:20.681593Z"},"trusted":true},"execution_count":319,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 486 entries, 0 to 485\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   donation_id         486 non-null    int64  \n 1   0_max               486 non-null    float64\n 2   0_window2_max       486 non-null    float64\n 3   0_window3_max       486 non-null    float64\n 4   0_window4_max       486 non-null    float64\n 5   0_window5_max       486 non-null    float64\n 6   0_window6_max       486 non-null    float64\n 7   0_window7_max       486 non-null    float64\n 8   0_window8_max       486 non-null    float64\n 9   0_window9_max       486 non-null    float64\n 10  0_window10_max      486 non-null    float64\n 11  0_idx_max           486 non-null    int64  \n 12  0_window2_idx_max   486 non-null    int64  \n 13  0_window3_idx_max   486 non-null    int64  \n 14  0_window4_idx_max   486 non-null    int64  \n 15  0_window5_idx_max   486 non-null    int64  \n 16  0_window6_idx_max   486 non-null    int64  \n 17  0_window7_idx_max   486 non-null    int64  \n 18  0_window8_idx_max   486 non-null    int64  \n 19  0_window9_idx_max   486 non-null    int64  \n 20  0_window10_idx_max  486 non-null    int64  \n 21  0_min               486 non-null    float64\n 22  0_window2_min       486 non-null    float64\n 23  0_window3_min       486 non-null    float64\n 24  0_window4_min       486 non-null    float64\n 25  0_window5_min       486 non-null    float64\n 26  0_window6_min       486 non-null    float64\n 27  0_window7_min       486 non-null    float64\n 28  0_window8_min       486 non-null    float64\n 29  0_window9_min       486 non-null    float64\n 30  0_window10_min      486 non-null    float64\n 31  0_idx_min           486 non-null    int64  \n 32  0_window2_idx_min   486 non-null    int64  \n 33  0_window3_idx_min   486 non-null    int64  \n 34  0_window4_idx_min   486 non-null    int64  \n 35  0_window5_idx_min   486 non-null    int64  \n 36  0_window6_idx_min   486 non-null    int64  \n 37  0_window7_idx_min   486 non-null    int64  \n 38  0_window8_idx_min   486 non-null    int64  \n 39  0_window9_idx_min   486 non-null    int64  \n 40  0_window10_idx_min  486 non-null    int64  \n 41  target              486 non-null    int64  \n 42  std                 486 non-null    float64\n 43  temperature_x_x     486 non-null    float64\n 44  humidity_x_x        486 non-null    float64\n 45  temperature_y_x     486 non-null    int64  \n 46  humidity_y_x        486 non-null    int64  \n 47  temperature_x_y     486 non-null    float64\n 48  humidity_x_y        486 non-null    float64\n 49  temperature_y_y     486 non-null    int64  \n 50  humidity_y_y        486 non-null    int64  \ndtypes: float64(25), int64(26)\nmemory usage: 197.4 KB\n","output_type":"stream"}]},{"cell_type":"code","source":"# train['id']\n# train.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:25:21.837283Z","iopub.execute_input":"2022-02-11T13:25:21.837548Z","iopub.status.idle":"2022-02-11T13:25:21.841308Z","shell.execute_reply.started":"2022-02-11T13:25:21.837519Z","shell.execute_reply":"2022-02-11T13:25:21.840169Z"},"trusted":true},"execution_count":320,"outputs":[]},{"cell_type":"code","source":"n_folds = 5\nExperiment_name = 'model1'\n'''creating folds '''\nos.makedirs(\"proc_data\", exist_ok=True)\ngroups_by_patient_id_list = train.donation_id.tolist()\ntry: \n    folds=pd.read_csv(\"./proc_data/folds_id.csv\")\n    train=train.merge(folds,on=\"donation_id\",how=\"left\")\n    train.fold.nunique()\nexcept: \n    from sklearn.model_selection import GroupKFold \n    kfold=GroupKFold(n_splits = n_folds) # change this random_state or all of you will have the same score  :D \n    y_labels = train['target']\n    train.reset_index(drop=True,inplace=True)\n    folds=train[[\"donation_id\"]].copy()\n    folds[\"fold\"]=0\n    for fold,(tr_indx,val_ind) in enumerate(kfold.split(train, y_labels, groups = groups_by_patient_id_list)) : \n        folds.loc[val_ind,\"fold\"]=fold\n    folds.to_csv(\"./proc_data/folds_id.csv\",index=False)\n    train=train.merge(folds,on=\"donation_id\",how=\"left\")\n\n    del folds\n\ntarget_name=\"target\"\nid_name=\"donation_id\"\nfeatures_to_remove=[target_name,id_name,\"fold\",\"donation_id\"]\nfeatures=train.columns.tolist()\nfeatures=[ f for f in  features if f not in features_to_remove  ]\n'''scaling the data'''\n# scaler = StandardScaler()\n# unscaled_train = train\n# train[features] = scaler.fit_transform(unscaled_train[features])\n# test_unscaled=test\n# test[features] = scaler.transform(test_unscaled[features])\n'''modelisation part'''\nfrom sklearn.metrics import accuracy_score\ndef metric(y,x):\n    return accuracy_score(x,y)\ndef train_function(model,train,test,params,other_params,target_name,features,metric):\n    folds_num=train.fold.nunique()\n    validation=train[[\"donation_id\",\"fold\",target_name]].copy()\n    validation[\"pred_\"+target_name]=0\n    sub=test[[\"donation_id\"]].copy()\n    sub[target_name]=0\n    for fold in np.sort(train.fold.unique()):\n        print(\"#\"*50+\" {} \".format(fold)+\"#\"*50)\n        os.makedirs(\"model_save/lgbm/{}/{}\".format(Experiment_name,str(int(fold))), exist_ok=True)\n        X_train=train[train.fold!=fold]\n        X_val=train[train.fold==fold]\n\n        train_pred,validation_pred,test_pred,model_save=model(X_train,X_val,test,params,other_params)\n#         print(.shape)\n        validation.loc[validation.fold==fold,\"pred_\"+target_name]= np.argmax(validation_pred,axis = 1)\n        sub[target_name]+=np.argmax(test_pred,axis = 1)/folds_num\n        train_score=metric(X_train[target_name],np.argmax(train_pred,axis = 1))\n        val_score=metric(X_val[target_name],np.argmax(validation_pred,axis = 1))\n        print(\"train score : {} validation score : {}\".format(round(train_score,4),round(val_score,4)))\n    final_validation_score=metric(validation[target_name],validation[\"pred_\"+target_name])\n    print(\"final validation score : {}\".format(final_validation_score))\n\n    return sub,validation,final_validation_score,model_save\n\ndef lgbm_model(X_train,X_val,X_test,params,other_params):\n    dtrain = lgbm.Dataset(data=X_train[features], label=X_train[target_name], feature_name=features)\n    dval = lgbm.Dataset(data=X_val[features], label=X_val[target_name], feature_name=features)\n\n    model = lgbm.train(\n        params=params,\n        train_set=dtrain,\n        num_boost_round=other_params[\"num_boost_round\"],\n        valid_sets=(dtrain, dval),\n        early_stopping_rounds=other_params[\"early_stopping_rounds\"],\n        verbose_eval=other_params[\"verbose_eval\"],\n    )        \n    best_iteration = model.best_iteration\n    train_pred=model.predict(X_train[features], num_iteration=best_iteration)\n    validation_pred=model.predict(X_val[features], num_iteration=best_iteration)\n    test_pred=model.predict(test[features], num_iteration=best_iteration)\n\n    return train_pred,validation_pred,test_pred,model\nother_params={\"num_boost_round\":10000,\n          \"early_stopping_rounds\":1000,\n          \"verbose_eval\":1000,\n}\n# n_classes = y_labels.unique().shape[0]\nlgbm_params = {'task': 'train',\n    'boosting_type': 'dart',\n    'objective': 'multiclassova',\n#     'num_class':3,\n#     'is_unbalance': True,\n    'metric': 'multi_error',\n    'learning_rate': 0.02296,\n    'max_depth': 17,\n    'num_leaves': 13,\n    'feature_fraction': 0.9,\n#     'importance_type':'gain',\n    'bagging_fraction': 0.6,\n    'bagging_freq': 17,\n    'class_weight' : 'balanced',\n#     \"bagging_fraction\": 0.2,\n#     \"bagging_freq\": 2,\n    \"num_classes\" :3,\n    \"verbosity\"  : -1,\n#     \"boosting_type\": \"gbdt\",\n#     \"feature_fraction\": 0.6,\n#     \"learning_rate\": 0.01,\n#     \"max_depth\": 5,\n#     \"num_threads\": 2,\n#     \"objective\": \"multiclassova\",\n#     \"metric\": \"accuracy\",\n    \"seed\": 2020\n#     \"lambda_l1\" : 15,\n#     \"lambda_l2\" : 4, # this was 5\n}\nsub,validation,score,model=train_function(model=lgbm_model,\n                                    train=train,\n                                    test=test,\n                                    params=lgbm_params,\n                                    other_params=other_params,\n                                    target_name=target_name,\n                                    features=features,\n                                    metric=metric)\n# return sub, validation, score, model","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:25:23.138307Z","iopub.execute_input":"2022-02-11T13:25:23.139381Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"################################################## 0 ##################################################\n[1000]\ttraining's multi_error: 0.0335052\tvalid_1's multi_error: 0.489796\n[2000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.469388\n[3000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.44898\n[4000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.469388\n[5000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.469388\n[6000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.489796\n[7000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.459184\n[8000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.44898\n[9000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.44898\n[10000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.459184\ntrain score : 1.0 validation score : 0.5408\n################################################## 1 ##################################################\n[LightGBM] [Warning] Unknown parameter: class_weight\n[1000]\ttraining's multi_error: 0.0205656\tvalid_1's multi_error: 0.546392\n[2000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.57732\n[3000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.546392\n[4000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.546392\n[5000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.556701\n[6000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.56701\n[7000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.56701\n[8000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.587629\n[9000]\ttraining's multi_error: 0\tvalid_1's multi_error: 0.56701\n","output_type":"stream"}]},{"cell_type":"code","source":"# train['target']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # validation\n# feature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,train[fea].columns)), columns=['Value','Feature'])\n\n# plt.figure(figsize=(20, 10))\n# sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n# plt.title('LightGBM Features (avg over folds)')\n# plt.tight_layout()\n# plt.show()\ncust_fea = pd.DataFrame({'features': train[features].columns ,\n                            'importances' : model.feature_importance() })\ncust_fea = cust_fea.loc[cust_fea.importances>0].features.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(cust_fea)\n(validation['pred_target']).unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[train.target ==0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}